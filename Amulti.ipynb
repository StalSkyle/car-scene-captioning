{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a7519-b644-4fb9-9435-9f5a88716eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "llama_multimodal_pipeline.py\n",
    "\n",
    "Пример интеграции мультимодальной LLaMA (LLaVA / LLaMA-мультимодал) в существующий пайплайн `autocaption`.\n",
    "\n",
    "Файл показывает:\n",
    "- как обернуть мультимодальную модель в класс `LlamaMultimodal` с двумя методами:\n",
    "    - `describe(image)` — генерирует несколько вариантов описания сцены\n",
    "    - `answer_question(image, question)` — возвращает ответ на вопрос (VQA)\n",
    "- как заменить существующие `PhotoDescriber` и `PhotoDescriberWithQuestion` на одну модель LLaMA\n",
    "\n",
    "ВНИМАНИЕ:\n",
    "- Конкретное имя модели (model_name) и способ загрузки зависят от того, какую реализацию мультимодальной LLaMA вы используете (LLaVA, MiniGPT-4, mLLaMA, etc.).\n",
    "- В коде показаны 2 возможных бэкенда: \"hf\" (Hugging Face) и \"api\" (локальный/удалённый API типа Ollama/OpenAI). Выберите подходящий и укажите корректные параметры.\n",
    "\n",
    "Зависимости (пример):\n",
    "    pip install torch torchvision transformers pillow requests\n",
    "\n",
    "Если у вас есть конкретная модель (ссылка на HF или endpoint Ollama), просто подставьте её в параметры при создании `LlamaMultimodal`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "from typing import Optional, Dict, Any, List\n",
    "from PIL import Image\n",
    "\n",
    "# Опционально: зависимости для HF\n",
    "try:\n",
    "    from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "    import torch\n",
    "    HF_AVAILABLE = True\n",
    "except Exception:\n",
    "    HF_AVAILABLE = False\n",
    "\n",
    "# Опционально: зависимости для API (requests)\n",
    "import requests\n",
    "\n",
    "\n",
    "class LlamaMultimodal:\n",
    "    \"\"\"Обёртка над мультимодальной LLaMA-реализацией.\n",
    "\n",
    "    Поддерживает два режима работы:\n",
    "      - mode='hf'  : загрузка модели через Hugging Face (локально / на сервере)\n",
    "      - mode='api' : использование HTTP API (например, Ollama или ваш собственный endpoint)\n",
    "\n",
    "    Настройте модель/endpoint под свою инфраструктуру.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode: str = \"hf\", model_name: Optional[str] = None, api_url: Optional[str] = None, device: str = \"cpu\"):\n",
    "        self.mode = mode\n",
    "        self.model_name = model_name\n",
    "        self.api_url = api_url\n",
    "        self.device = device\n",
    "\n",
    "        if mode == \"hf\":\n",
    "            if not HF_AVAILABLE:\n",
    "                raise RuntimeError(\"transformers/torch недоступны — установите зависимости для HF режима\")\n",
    "            if model_name is None:\n",
    "                raise ValueError(\"Укажите имя модели model_name для HF режима (модель должна поддерживать vision+LLM)\")\n",
    "\n",
    "            # Попытка загрузить процессор/модель. Пользователь должен указать корректную мультимодальную модель.\n",
    "            # Пример (замените на вашу модель): model_name = \"your-multimodal-llama-model\"\n",
    "            print(f\"Загружаем HF модель: {model_name} ...\")\n",
    "            try:\n",
    "                self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "                # Many multimodal HF models for LLaMA variants use AutoModelForCausalLM (или специализированный класс)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                # Перекладываем на устройство\n",
    "                if torch.cuda.is_available() and self.device.startswith(\"cuda\"):\n",
    "                    self.model.to(self.device)\n",
    "                print(\"Модель HF загружена.\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Не удалось загрузить HF модель: {e}\")\n",
    "\n",
    "        elif mode == \"api\":\n",
    "            if api_url is None:\n",
    "                raise ValueError(\"Укажите api_url для режима 'api' (например, http://localhost:11434 или ваш endpoint)\")\n",
    "            print(f\"Использовать API endpoint: {api_url}\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"mode должен быть 'hf' или 'api'\")\n",
    "\n",
    "    def _pil_to_base64(self, image: Image.Image) -> str:\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    def describe(self, image: Image.Image, max_length: int = 256) -> Dict[str, str]:\n",
    "        \"\"\"Возвращает словарь с несколькими текстовыми описаниями.\n",
    "\n",
    "        Формат результата:\n",
    "            {\n",
    "                'base': ...,\n",
    "                'detailed': ...,\n",
    "                'alternative': ...\n",
    "            }\n",
    "        \"\"\"\n",
    "        prompt_base = (\n",
    "            \"Опиши кратко, что изображено на фото (1-2 фразы).\\n\"\n",
    "            \"Далее — подробное описание сцены и элементов (3-4 предложения).\\n\"\n",
    "            \"Наконец — альтернативная формулировка краткого описания.\\n\"\n",
    "            \"Формат вывода: \\nBASE: <...>\\nDETAILED: <...>\\nALTERNATIVE: <...>\\n\"\n",
    "        )\n",
    "\n",
    "        response_text = self._run_multimodal_prompt(image, prompt_base, max_length=max_length)\n",
    "\n",
    "        # Парсим ответ по меткам. Если модель вернула произвольный текст — делаем best-effort разбор.\n",
    "        parts = {\"base\": \"\", \"detailed\": \"\", \"alternative\": \"\"}\n",
    "        try:\n",
    "            # Ищем заранее ожидаемые маркеры\n",
    "            for line in response_text.splitlines():\n",
    "                if line.strip().upper().startswith(\"BASE:\"):\n",
    "                    parts['base'] = line.split(':', 1)[1].strip()\n",
    "                elif line.strip().upper().startswith(\"DETAILED:\"):\n",
    "                    parts['detailed'] = line.split(':', 1)[1].strip()\n",
    "                elif line.strip().upper().startswith(\"ALTERNATIVE:\"):\n",
    "                    parts['alternative'] = line.split(':', 1)[1].strip()\n",
    "\n",
    "            # Если не разобрали — делим ответ на блоки по пустым строкам\n",
    "            if not parts['base'] and not parts['detailed'] and not parts['alternative']:\n",
    "                blocks = [b.strip() for b in response_text.split('\\n\\n') if b.strip()]\n",
    "                if len(blocks) >= 1:\n",
    "                    parts['base'] = blocks[0]\n",
    "                if len(blocks) >= 2:\n",
    "                    parts['detailed'] = blocks[1]\n",
    "                if len(blocks) >= 3:\n",
    "                    parts['alternative'] = blocks[2]\n",
    "\n",
    "        except Exception:\n",
    "            parts['base'] = response_text\n",
    "\n",
    "        # Гарантии: если какое-то поле пустое, используем полный текст\n",
    "        if not parts['base']:\n",
    "            parts['base'] = response_text.split('\\n')[0][:200]\n",
    "        if not parts['detailed']:\n",
    "            parts['detailed'] = response_text[:1000]\n",
    "        if not parts['alternative']:\n",
    "            parts['alternative'] = parts['base']\n",
    "\n",
    "        return parts\n",
    "\n",
    "    def answer_question(self, image: Image.Image, question: str, max_length: int = 128) -> str:\n",
    "        \"\"\"Отвечает на заданный вопрос про изображение.\n",
    "\n",
    "        question — строка на любом языке (желательно на одном языке с системным промптом модели).\n",
    "        \"\"\"\n",
    "        prompt = f\"Вопрос: {question}\\nКороткий и конкретный ответ:\"\n",
    "        response_text = self._run_multimodal_prompt(image, prompt, max_length=max_length)\n",
    "        return response_text.strip()\n",
    "\n",
    "    def _run_multimodal_prompt(self, image: Image.Image, prompt: str, max_length: int = 256) -> str:\n",
    "        if self.mode == \"hf\":\n",
    "            # Универсальный пример: процессор принимает images и текстовые подсказки.\n",
    "            # Конкретные детали зависят от процессора/модели.\n",
    "            try:\n",
    "                inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "                # Перемещаем тензоры на устройство модели\n",
    "                if hasattr(inputs, 'to'):\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "                # Генерация — зависит от реализации модели (некоторые мультимодальные модели используют generate)\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(**inputs, max_new_tokens=max_length)\n",
    "                text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # Некоторые модели возвращают исходный промпт + ответ — уберём промпт, если он присутствует\n",
    "                if prompt.strip() in text:\n",
    "                    text = text.split(prompt, 1)[-1].strip()\n",
    "                return text\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Ошибка при генерации HF: {e}\")\n",
    "\n",
    "        elif self.mode == \"api\":\n",
    "            # Пример протокола для API: отправляем image как base64 + текстовый промпт.\n",
    "            # Ожидается, что endpoint вернёт JSON {\"text\": \"...\"}\n",
    "            b64 = self._pil_to_base64(image)\n",
    "            payload = {\n",
    "                \"prompt\": prompt,\n",
    "                \"image_base64\": b64,\n",
    "                \"max_tokens\": max_length\n",
    "            }\n",
    "            try:\n",
    "                r = requests.post(self.api_url, json=payload, timeout=60)\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                # Предполагается поле 'text' или 'output'\n",
    "                return data.get('text') or data.get('output') or json.dumps(data)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Ошибка при обращении к API: {e}\")\n",
    "        else:\n",
    "            raise RuntimeError(\"Неподдерживаемый режим\")\n",
    "\n",
    "\n",
    "# --- Интеграция с имеющимся pipeline ---\n",
    "# Ниже пример адаптации вашей функции run_pipeline: заменяем describer и q_describer\n",
    "\n",
    "from autocaption import ImageLoader, ImageRotator, CarDetector, ObjectExtractor, SceneExtractor\n",
    "\n",
    "\n",
    "def run_pipeline_with_llama(image_path: List[str], source: bool, llama: LlamaMultimodal) -> List[List[Any]]:\n",
    "    res = [[] for _ in range(len(image_path))]\n",
    "\n",
    "    # инициализация классов (как у вас)\n",
    "    print('Инициализация модели поворота')\n",
    "    rotator = ImageRotator()\n",
    "    print('Инициализация модели детектора автомобилей')\n",
    "    car_detector = CarDetector()\n",
    "    print('Инициализация модели нахождения объектов')\n",
    "    object_extractor = ObjectExtractor()\n",
    "    print('Инициализация модели классификации сцены')\n",
    "    scene_classificator = SceneExtractor()\n",
    "\n",
    "    print('Начинается обработка изображений (с LLaMA)')\n",
    "    for i, path in enumerate(image_path):\n",
    "        print(f\"Обрабатывается изображение №{i+1}\")\n",
    "        image = ImageLoader(path=path, source=source).load_image()\n",
    "\n",
    "        if image is None:\n",
    "            res[i].append(f\"Не удалось загрузить изображение: {path}.\")\n",
    "            continue\n",
    "\n",
    "        image = rotator.rotate_image(image)\n",
    "        if not car_detector.detect_car(image):\n",
    "            res[i].append(f\"Не удалось определить наличие автомобиля на изображении: {path}.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Нахождение признаков\n",
    "        res[i].append(object_extractor.extract_features(image))\n",
    "\n",
    "        temp_dict = scene_classificator.predict_scene(image)\n",
    "        temp_dict.popitem()\n",
    "        res[i].append(temp_dict)\n",
    "\n",
    "        # --- заменяем генераторы описаний ---\n",
    "        try:\n",
    "            llama_desc = llama.describe(image)\n",
    "            res[i].append(f\"Базовое описание: {llama_desc.get('base')}\")\n",
    "            res[i].append(f\"Подробное описание: {llama_desc.get('detailed')}\")\n",
    "            res[i].append(f\"Альтернативное описание: {llama_desc.get('alternative')}\")\n",
    "\n",
    "            # VQA — пример вопроса. Вы можете менять вопрос в зависимости от задачи.\n",
    "            vqa_q = \"Где находится автомобиль? (коротко)\"\n",
    "            vqa_answer = llama.answer_question(image, vqa_q)\n",
    "            res[i].append(f\"Описание по вопросу (VQA): {vqa_answer}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            res[i].append(f\"Ошибка при генерации описания LLaMA: {e}\")\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "# --- Пример использования ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Пример: HF режим (локальная загрузка) — замените model_name\n",
    "    llama = LlamaMultimodal(mode='hf', model_name='your-multimodal-llama-model', device='cuda:0')\n",
    "\n",
    "    # Пример: API режим (локальный Ollama / удалённый endpoint)\n",
    "    llama = LlamaMultimodal(mode='api', api_url='http://localhost:8080/generate')\n",
    "\n",
    "    # Пока просто создаём заглушку — пользователь должен выбрать конфигурацию\n",
    "    print(\"Этот модуль демонстрирует, как подключить мультимодальную LLaMA к pipeline.\\n\"\n",
    "          \"Задайте режим и модель в блоке '__main__' перед запуском.\")\n",
    "\n",
    "\n",
    "# Конец файла\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
