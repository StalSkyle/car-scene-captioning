# Car Scene Captioning

Проект автоматически генерирует текстовое описание окружения автомобиля по фотографии. Это решение помогает локализовать автомобиль в случаях, когда GPS-сигнал недоступен или неточен — например, при поиске арендованного автомобиля.

## Задача

Когда автомобиль теряется из-за проблем с GPS, для его поиска используются фотографии, сделанные пользователями или сотрудниками поддержки. Однако человеку, плохо ориентирующемуся на местности, может быть сложно определить местоположение машины только по изображению. Текстовое описание окружения (например, «рядом с детской площадкой», «под мостом», «около дома с табличкой адреса») позволяет сопоставить сцену с картой и ускорить поиск.

Цель проекта — создать конвейер моделей, который:
- проверяет наличие автомобиля на изображении,
- нормализует ориентацию изображения,
- извлекает объекты и тип сцены,
- генерирует краткое и информативное описание окружающей среды без упоминания самого автомобиля.

## Возможности

- Поддержка локальных изображений и изображений по URL.
- Автоматический поворот изображения для корректной ориентации.
- Детекция автомобиля (включая грузовики).
- Извлечение объектов с помощью YOLOv8 (на основе датасета Open Images).
- Классификация типа сцены (например: «parking lot», «city street», «highway»).
- Генерация описаний с помощью модели BLIP:
  - базовое, подробное и альтернативное описания;
  - специализированный запрос через BLIP-VQA с фокусом на окружение (без упоминания машины и неба).
- Опциональная постобработка с использованием LLaVA для формирования финального описания (реализована в `TextGenerator`).

## Архитектура конвейера (создана отдельная библиотека autocaption)

1. **ImageLoader** — загружает изображение из локального пути или URL.
2. **ImageRotator** — корректирует ориентацию изображения с помощью ResNet50.
3. **CarDetector** — проверяет наличие автомобиля или грузовика (YOLOv8).
4. **ObjectExtractor** — детектирует объекты на сцене (YOLOv8, предобученная на Open Images v7).
5. **SceneExtractor** — классифицирует тип сцены (ResNet18, обученная на BDD100K).
6. **PhotoDescriber** — генерирует три варианта описания с помощью BLIP (после удаления машины и неба через inpainting).
7. **PhotoDescriberWithQuestion** — использует BLIP-VQA с целевым вопросом для фокусировки на окружении.
8. **TextGenerator** — (опционально) агрегирует признаки и генерирует финальное описание с помощью LLaVA.

## Требования

- Python ≥ 3.8
- PyTorch ≥ 1.10
- Библиотеки:
  - `Pillow`, `opencv-python`, `requests`
  - `transformers`, `timm`
  - `ultralytics` (для YOLOv8)
  - `accelerate` (опционально, для BLIP)
- Доступ к GPU рекомендуется, но не обязателен.

## Установка

1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/StalSkyle/car-scene-captioning.git
   cd car-scene-captioning```

2. Установите зависимости:
    ```bash
    pip install -r requirements.txt```

3. При первом запуске PhotoDescriberWithQuestion автоматически клонирует официальный репозиторий BLIP в папку MODELS/BLIP. При необходимости измените путь в коде.

## Запуск

Пример использования:

```python
from main import run_pipeline

results = run_pipeline(
    image_path=[
        "/path/to/image1.jpg",
        "https://example.com/car.jpg"
    ],
    source=True  # True = локальный путь, False = URL
)

for i, res in enumerate(results):
    print(f"\n--- Результаты для изображения {i+1} ---")
    for item in res:
        print(item)
```

## Структура проекта
```bash
car-scene-captioning/
├── main.py                     # точка входа
├── autocaption/
│   ├── __init__.py
│   ├── image_loader.py         # загрузка изображений
│   ├── image_processor.py      # поворот и детекция авто
│   ├── feature_extractor.py    # извлечение объектов, сцены, описаний
│   └── text_generator.py       # (опционально) финальная генерация текста
└── MODELS/                     # папка для весов моделей
```

## Используемые модели и источники

- **BLIP (Bootstrapping Language-Image Pre-training)**
  Официальный репозиторий: [Salesforce/BLIP](https://github.com/salesforce/BLIP)
  Публикация: Junnan Li et al., *BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation*, ICML 2022.
  Используется для генерации описаний изображений и визуального ответа на вопросы (VQA).

- **YOLOv8**
  Разработчик: [Ultralytics](https://github.com/ultralytics/ultralytics)
  Используется для детекции автомобилей и других объектов на сцене. Модель `yolov8x-oiv7.pt` предобучена на датасете Open Images v7.

- **ResNet**
  Реализация из `torchvision`.
  - ResNet50 — используется для коррекции ориентации изображения.
  - ResNet18 — применяется для классификации типа сцены (обучена на датасете BDD100K).

- **Mask2Former**
  Доступна через библиотеку [Hugging Face Transformers](https://huggingface.co/docs/transformers/model_doc/mask2former).
  Используется для паноптической сегментации с целью удаления автомобиля и неба перед генерацией описания.

- **Датасеты**
  - **Open Images v7** — для детекции объектов.
  - **BDD100K** — для классификации городских сцен (улицы, парковки, шоссе и др.).
  - **LAION / CC3M / SBU** — использовались в предобучении BLIP (косвенно, через загруженные веса).

## Лицензия
Проект распространяется без лицензии. Используемые сторонние модели подчиняются своим лицензиям (см. соответствующие репозитории).
