{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47eb3612-c971-44c7-9647-ce5e1603fba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T13:03:05.617195Z",
     "iopub.status.busy": "2025-10-16T13:03:05.616077Z",
     "iopub.status.idle": "2025-10-16T13:03:06.395058Z",
     "shell.execute_reply": "2025-10-16T13:03:06.393900Z",
     "shell.execute_reply.started": "2025-10-16T13:03:05.617159Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pip is disabled in bash. Please use %pip magic.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Process exited with code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3901/267440198.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Рекомендуемое окружение: Python 3.8+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch torchvision transformers pillow numpy ultralytics tqdm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Если планируете BLIP VQA из репозитория salesforce/BLIP:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install git+https://github.com/salesforce/BLIP.git@main'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# (опционально) если хотите yolov8:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/ml_kernel/kernel.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScriptExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_output_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_user_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/ml_kernel/script_executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, lang, code)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Process exited with code %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: Process exited with code 1"
     ]
    }
   ],
   "source": [
    "# Рекомендуемое окружение: Python 3.8+\n",
    "!pip install torch torchvision transformers pillow numpy ultralytics tqdm\n",
    "# Если планируете BLIP VQA из репозитория salesforce/BLIP:\n",
    "!pip install git+https://github.com/salesforce/BLIP.git@main\n",
    "# (опционально) если хотите yolov8:\n",
    "!pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774aa68b-8c31-4a25-a314-9c0de785dab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T13:10:08.122719Z",
     "iopub.status.busy": "2025-10-16T13:10:08.121483Z",
     "iopub.status.idle": "2025-10-16T13:15:05.182132Z",
     "shell.execute_reply": "2025-10-16T13:15:05.181340Z",
     "shell.execute_reply.started": "2025-10-16T13:10:08.122675Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading BLIP captioning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8c545b3d-7987-4e8c-9fdc-1e3dc6616f68)')' thrown while requesting HEAD https://huggingface.co/Salesforce/blip-image-captioning-base/resolve/main/preprocessor_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DETR (object detection)...\n",
      "YOLO установлен, но путь к модели не найден или модель не указана: ../MODELS/yolov8x-oiv7.pt\n",
      "BLIP VQA недоступен (BLIP repo не найден). VQA будет выполнен через BLIP captioning fallback.\n",
      "Loading rotation model (resnet50)...\n",
      "Найдено изображений: 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 1/179 [00:01<04:18,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/179] img_00000.jpeg\n",
      "  Base: a white van parked on the side of a road\n",
      "  VQA: a white van parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 2/179 [00:02<03:51,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/179] img_00001.jpeg\n",
      "  Base: a bus driving down a street next to a curb\n",
      "  VQA: a white bus driving down the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 3/179 [00:03<03:45,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/179] img_00002.jpeg\n",
      "  Base: a white van parked on the side of the road\n",
      "  VQA: a white van parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 4/179 [00:05<03:53,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/179] img_00003.jpeg\n",
      "  Base: a white van parked on the side of the road\n",
      "  VQA: a white van parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 5/179 [00:06<03:57,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/179] img_00004.jpeg\n",
      "  Base: an abstract image of a tree in the woods\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 6/179 [00:08<03:58,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/179] img_00005.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▍         | 7/179 [00:09<04:11,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/179] img_00006.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▍         | 8/179 [00:11<03:58,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/179] img_00007.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a car parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   5%|▌         | 9/179 [00:12<03:59,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/179] img_00008.jpeg\n",
      "  Base: a man in a suit and tie walking down a street\n",
      "  VQA: a man in a suit and tie walking down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▌         | 10/179 [00:13<03:58,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/179] img_00010.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▌         | 11/179 [00:15<03:59,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/179] img_00011.jpeg\n",
      "  Base: a blurry image of a tree in the woods\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 12/179 [00:16<03:57,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/179] img_00012.jpeg\n",
      "  Base: a person riding a bike down a street\n",
      "  VQA: a person riding a bike on a city street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 13/179 [00:18<03:57,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/179] img_00013.jpeg\n",
      "  Base: a woman in a black dress standing in front of a wall\n",
      "  VQA: a woman with long hair standing in front of a wall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 14/179 [00:19<04:02,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/179] img_00014.jpeg\n",
      "  Base: a person walking down the street with their feet on the ground\n",
      "  VQA: a person walking down the street with their feet on the ground\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 15/179 [00:21<04:13,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/179] img_00016.jpeg\n",
      "  Base: a police car is parked on the side of the road\n",
      "  VQA: a police car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   9%|▉         | 16/179 [00:22<04:04,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/179] img_00017.jpeg\n",
      "  Base: an image of a black background with a white border\n",
      "  VQA: a close up of a black surface with a white background\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   9%|▉         | 17/179 [00:24<04:03,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/179] img_00018.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|█         | 18/179 [00:25<03:52,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/179] img_00019.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a car parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  11%|█         | 19/179 [00:27<03:50,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/179] img_00020.jpeg\n",
      "  Base: a close up of a black curtain with light coming through it\n",
      "  VQA: a close up of a black curtain with light coming through\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  11%|█         | 20/179 [00:28<03:50,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/179] img_00021.jpeg\n",
      "  Base: a car driving down a street at night\n",
      "  VQA: an image of a man in a suit and tie\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▏        | 21/179 [00:29<03:44,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/179] img_00022.jpeg\n",
      "  Base: a black and white background with vertical lines\n",
      "  VQA: a black and white wallpaper with vertical lines\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▏        | 22/179 [00:31<03:48,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/179] img_00023.jpeg\n",
      "  Base: a red car parked on the side of a road\n",
      "  VQA: a red car parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 23/179 [00:32<03:45,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/179] img_00024.jpeg\n",
      "  Base: a black and white photo of a forest\n",
      "  VQA: a black and white abstract background with vertical lines\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 24/179 [00:34<03:37,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/179] img_00025.jpeg\n",
      "  Base: a car driving down the road in a tunnel\n",
      "  VQA: a car driving down the road in a tunnel\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  14%|█▍        | 25/179 [00:35<03:41,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/179] img_00026.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▍        | 26/179 [00:37<03:36,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/179] img_00027.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▌        | 27/179 [00:38<03:41,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/179] img_00030.jpeg\n",
      "  Base: a person walking down a sidewalk with their feet on the ground\n",
      "  VQA: a skateboarder rides down the street on his skateboard\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  16%|█▌        | 28/179 [00:39<03:32,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28/179] img_00031.jpeg\n",
      "  Base: a person riding a skateboard down a street\n",
      "  VQA: a skateboarder rides down the street in the city\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  16%|█▌        | 29/179 [00:41<03:32,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/179] img_00032.jpeg\n",
      "  Base: a man in a suit standing in front of a wall\n",
      "  VQA: a man in a suit standing in front of a wall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 30/179 [00:42<03:38,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/179] img_00033.jpeg\n",
      "  Base: a man in a suit and tie standing next to a wall\n",
      "  VQA: a man in a suit and tie standing next to a wall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 31/179 [00:44<03:39,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/179] img_00034.jpeg\n",
      "  Base: a man in a suit standing in front of a wall\n",
      "  VQA: a man in a suit standing in front of a wall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█▊        | 32/179 [00:46<03:43,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/179] img_00035.jpeg\n",
      "  Base: a man in a suit and tie standing next to a wall\n",
      "  VQA: a man in a suit and tie standing next to a wall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█▊        | 33/179 [00:47<03:31,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/179] img_00036.jpeg\n",
      "  Base: a black and white background with a silver stripe\n",
      "  VQA: a black and white background with a silver stripe\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▉        | 34/179 [00:48<03:28,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34/179] img_00037.jpeg\n",
      "  Base: a man riding a skateboard down a street\n",
      "  VQA: the shadow of a person on a skateboard\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|█▉        | 35/179 [00:50<03:22,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35/179] img_00038.jpeg\n",
      "  Base: a man riding a skateboard down a street\n",
      "  VQA: a man riding a skateboard down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|██        | 36/179 [00:51<03:30,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/179] img_00039.jpeg\n",
      "  Base: an image of a man in the middle of a forest\n",
      "  VQA: an image of a man in the middle of a forest\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  21%|██        | 37/179 [00:53<03:27,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37/179] img_00040.jpeg\n",
      "  Base: a close up of a piece of wood on the ground\n",
      "  VQA: a close up of a piece of wood next to a brick\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  21%|██        | 38/179 [00:54<03:22,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38/179] img_00041.jpeg\n",
      "  Base: a black and white photo of a man in the woods\n",
      "  VQA: a black and white photo of a forest with trees\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  22%|██▏       | 39/179 [00:56<03:25,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39/179] img_00042.jpeg\n",
      "  Base: a man in a suit and tie standing in front of a wall\n",
      "  VQA: a man in a suit and tie standing in front of a wall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  22%|██▏       | 40/179 [00:57<03:25,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/179] img_00043.jpeg\n",
      "  Base: a close up view of the surface of the moon\n",
      "  VQA: a close up view of the surface of the moon\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 41/179 [00:58<03:19,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41/179] img_00044.jpeg\n",
      "  Base: a car parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 42/179 [01:00<03:20,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42/179] img_00045.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  24%|██▍       | 43/179 [01:01<03:16,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43/179] img_00046.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▍       | 44/179 [01:03<03:09,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44/179] img_00047.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 45/179 [01:04<03:10,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45/179] img_00048.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  26%|██▌       | 46/179 [01:05<03:05,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46/179] img_00049.jpeg\n",
      "  Base: a cat is sitting on the side of the road\n",
      "  VQA: a car parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  26%|██▋       | 47/179 [01:07<02:59,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47/179] img_00050.jpeg\n",
      "  Base: an abstract image of trees in the woods\n",
      "  VQA: an abstract image of trees in the woods\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|██▋       | 48/179 [01:08<02:58,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48/179] img_00052.jpeg\n",
      "  Base: the shadow of a person standing on the ground\n",
      "  VQA: the shadow of a person standing on a dirt road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|██▋       | 49/179 [01:10<02:58,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49/179] img_00053.jpeg\n",
      "  Base: a car parked on the side of a road at night\n",
      "  VQA: a car parked on the side of a road at night\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 50/179 [01:11<02:59,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/179] img_00054.jpeg\n",
      "  Base: a black and white photo of a man in a suit and tie\n",
      "  VQA: a brown paper bag on a black background\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 51/179 [01:12<03:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51/179] img_00055.jpeg\n",
      "  Base: a car driving down a street at night\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  29%|██▉       | 52/179 [01:14<02:59,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52/179] img_00057.jpeg\n",
      "  Base: a black and white photo of a woman's face\n",
      "  VQA: a black and white photo of a woman's face\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|██▉       | 53/179 [01:15<03:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53/179] img_00058.jpeg\n",
      "  Base: a black and white photo of a black and white background\n",
      "  VQA: a black and white photo of a black and white background\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|███       | 54/179 [01:17<02:58,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54/179] img_00059.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  31%|███       | 55/179 [01:18<02:59,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55/179] img_00062.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  31%|███▏      | 56/179 [01:20<03:00,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56/179] img_00064.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 57/179 [01:21<02:54,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/179] img_00065.jpeg\n",
      "  Base: a person is walking down the stairs in the dark\n",
      "  VQA: a person standing in the middle of a tunnel\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 58/179 [01:22<02:49,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58/179] img_00066.jpeg\n",
      "  Base: a man riding a bike down a city street\n",
      "  VQA: a man riding a bike down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 59/179 [01:24<02:47,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59/179] img_00067.jpeg\n",
      "  Base: an image of a man in a suit and tie\n",
      "  VQA: a man in a suit and tie walking down the street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  34%|███▎      | 60/179 [01:25<02:45,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60/179] img_00068.jpeg\n",
      "  Base: a person riding a motorcycle down a road\n",
      "  VQA: a person riding a motorcycle down a dirt road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  34%|███▍      | 61/179 [01:27<02:47,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61/179] img_00069.jpeg\n",
      "  Base: a black and white photo of a black and white photo\n",
      "  VQA: a close up of a black and white background\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▍      | 62/179 [01:28<02:48,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62/179] img_00070.jpeg\n",
      "  Base: a close up of a metal tube\n",
      "  VQA: a close up of a metal tube with a brown background\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▌      | 63/179 [01:29<02:40,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63/179] img_00071.jpeg\n",
      "  Base: a car is submerged in a flooded street\n",
      "  VQA: a car is submerged in a flooded street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  36%|███▌      | 64/179 [01:31<02:41,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64/179] img_00073.jpeg\n",
      "  Base: a police car is parked on the side of the road\n",
      "  VQA: a police car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  36%|███▋      | 65/179 [01:32<02:41,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65/179] img_00074.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car parked on the side of a road at night\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  37%|███▋      | 66/179 [01:34<02:38,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66/179] img_00076.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  37%|███▋      | 67/179 [01:35<02:37,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67/179] img_00078.jpeg\n",
      "  Base: a man riding a skateboard down a street\n",
      "  VQA: a person riding on a skateboard down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 68/179 [01:37<02:37,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68/179] img_00079.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  39%|███▊      | 69/179 [01:38<02:33,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69/179] img_00080.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a truck parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  39%|███▉      | 70/179 [01:39<02:36,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70/179] img_00081.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a black car parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|███▉      | 71/179 [01:41<02:35,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71/179] img_00082.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|████      | 72/179 [01:42<02:33,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72/179] img_00084.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  41%|████      | 73/179 [01:44<02:41,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73/179] img_00085.jpeg\n",
      "  Base: a dark blue background with horizontal lines\n",
      "  VQA: a purple and black background with a white border\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  41%|████▏     | 74/179 [01:45<02:34,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74/179] img_00086.jpeg\n",
      "  Base: an image of the moon in the night sky\n",
      "  VQA: an image of the moon in the night sky\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 75/179 [01:47<02:28,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75/179] img_00087.jpeg\n",
      "  Base: the shadow of a person walking down a street\n",
      "  VQA: the shadow of a person walking down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 76/179 [01:48<02:24,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76/179] img_00088.jpeg\n",
      "  Base: an asteroid is seen in this image taken by nasa astronauts\n",
      "  VQA: an asteroid is seen in this image taken from nasa\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|████▎     | 77/179 [01:49<02:24,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77/179] img_00089.jpeg\n",
      "  Base: a man riding a skateboard down a street\n",
      "  VQA: a man riding a skateboard down the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  44%|████▎     | 78/179 [01:51<02:24,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78/179] img_00090.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  44%|████▍     | 79/179 [01:52<02:24,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79/179] img_00091.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|████▍     | 80/179 [01:54<02:20,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80/179] img_00092.jpeg\n",
      "  Base: a man in a suit and tie standing in a dark room\n",
      "  VQA: a man in a suit and tie with a tie\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|████▌     | 81/179 [01:55<02:17,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81/179] img_00093.jpeg\n",
      "  Base: a car is parked on the street at night\n",
      "  VQA: a car with a license plate on it's back\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  46%|████▌     | 82/179 [01:57<02:15,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82/179] img_00094.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  46%|████▋     | 83/179 [01:58<02:09,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83/179] img_00095.jpeg\n",
      "  Base: an image of a black and brown background\n",
      "  VQA: a black and white background with a brown stripe\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|████▋     | 84/179 [01:59<02:09,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84/179] img_00096.jpeg\n",
      "  Base: a truck is parked on the side of the road\n",
      "  VQA: a car that is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|████▋     | 85/179 [02:01<02:08,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85/179] img_00097.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 86/179 [02:02<02:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86/179] img_00098.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  49%|████▊     | 87/179 [02:03<02:09,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87/179] img_00099.jpeg\n",
      "  Base: a car parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  49%|████▉     | 88/179 [02:05<02:09,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88/179] img_00100.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|████▉     | 89/179 [02:06<02:07,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89/179] img_00101.jpeg\n",
      "  Base: a white car parked on the side of the road\n",
      "  VQA: a white car parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 90/179 [02:08<02:02,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90/179] img_00102.jpeg\n",
      "  Base: the shadow of a person walking on a sidewalk\n",
      "  VQA: the shadow of a person walking on a sidewalk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  51%|█████     | 91/179 [02:09<01:58,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91/179] img_00103.jpeg\n",
      "  Base: a tire on the side of a road\n",
      "  VQA: a tire on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  51%|█████▏    | 92/179 [02:10<01:55,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92/179] img_00104.jpeg\n",
      "  Base: a car driving through a tunnel in the rain\n",
      "  VQA: a car driving through a tunnel in the rain\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 93/179 [02:12<01:55,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93/179] img_00105.jpeg\n",
      "  Base: a black cat sitting on the side of a road\n",
      "  VQA: a black cat sitting on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  53%|█████▎    | 94/179 [02:13<01:56,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94/179] img_00106.jpeg\n",
      "  Base: a red car parked on the side of a road\n",
      "  VQA: a red car parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  53%|█████▎    | 95/179 [02:14<01:58,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95/179] img_00107.jpeg\n",
      "  Base: a car driving through a tunnel on a road\n",
      "  VQA: a car driving through a tunnel on a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▎    | 96/179 [02:16<01:58,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96/179] img_00108.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▍    | 97/179 [02:17<01:56,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97/179] img_00109.jpeg\n",
      "  Base: a close up of a black and white carpet\n",
      "  VQA: a person walking down a sidewalk with a cell phone\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▍    | 98/179 [02:19<01:54,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98/179] img_00110.jpeg\n",
      "  Base: a close up of a skateboard on the ground\n",
      "  VQA: a close up of a skateboard on the ground\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▌    | 99/179 [02:20<01:54,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/179] img_00111.jpeg\n",
      "  Base: a person walking down a sidewalk with their feet on the ground\n",
      "  VQA: a person walking down a sidewalk with their feet on the ground\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|█████▌    | 100/179 [02:22<01:51,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/179] img_00112.jpeg\n",
      "  Base: a close up of a skateboard on the ground\n",
      "  VQA: a close up of a skateboard on the ground\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|█████▋    | 101/179 [02:23<01:49,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101/179] img_00113.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  57%|█████▋    | 102/179 [02:24<01:49,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102/179] img_00114.jpeg\n",
      "  Base: a skateboarder riding down the street with his board\n",
      "  VQA: a skateboarder riding down the street with his board\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 103/179 [02:26<01:47,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103/179] img_00115.jpeg\n",
      "  Base: a person standing on a sidewalk next to a car\n",
      "  VQA: a person standing on a sidewalk next to a car\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 104/179 [02:27<01:45,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104/179] img_00116.jpeg\n",
      "  Base: two bags sitting on the grass next to each other bags\n",
      "  VQA: a man sitting in the grass next to a bag\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  59%|█████▊    | 105/179 [02:29<01:42,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105/179] img_00117.jpeg\n",
      "  Base: an image of a black and white striped background\n",
      "  VQA: a close up of a black and white striped background\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  59%|█████▉    | 106/179 [02:30<01:41,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106/179] img_00118.jpeg\n",
      "  Base: an image of a black and white striped background\n",
      "  VQA: a black and white image of a black and white image\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|█████▉    | 107/179 [02:31<01:39,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107/179] img_00119.jpeg\n",
      "  Base: a black cat is laying on the ground\n",
      "  VQA: a black and white cat laying on the ground\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|██████    | 108/179 [02:33<01:41,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[108/179] img_00120.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a person on a skateboard\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  61%|██████    | 109/179 [02:34<01:37,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109/179] img_00121.jpeg\n",
      "  Base: a man riding a motorcycle down a street\n",
      "  VQA: a man riding a motorcycle down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  61%|██████▏   | 110/179 [02:36<01:39,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110/179] img_00122.jpeg\n",
      "  Base: a man in a suit and tie standing in front of a wall\n",
      "  VQA: a man in a suit and tie standing in front of a wall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▏   | 111/179 [02:37<01:34,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111/179] img_00123.jpeg\n",
      "  Base: a water fountain in the middle of a park\n",
      "  VQA: a water fountain in the middle of a park\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 112/179 [02:38<01:34,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112/179] img_00124.jpeg\n",
      "  Base: a man in a suit and tie walking down the street\n",
      "  VQA: a man in a suit and tie walking down the street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 113/179 [02:40<01:31,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113/179] img_00125.jpeg\n",
      "  Base: a man riding a skateboard down a street\n",
      "  VQA: a person riding on a skateboard down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  64%|██████▎   | 114/179 [02:41<01:29,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114/179] img_00126.jpeg\n",
      "  Base: a car driving down a street next to a tall building\n",
      "  VQA: a car driving through a tunnel on a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  64%|██████▍   | 115/179 [02:43<01:29,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115/179] img_00128.jpeg\n",
      "  Base: a man in a suit standing in front of a door\n",
      "  VQA: a man in a suit standing in front of a door\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▍   | 116/179 [02:44<01:29,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116/179] img_00129.jpeg\n",
      "  Base: a car that is upside on the side of the road\n",
      "  VQA: a car that is upside on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▌   | 117/179 [02:45<01:28,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117/179] img_00130.jpeg\n",
      "  Base: a white motorcycle parked on the side of a road\n",
      "  VQA: a motorcycle parked on the side of a road at night\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  66%|██████▌   | 118/179 [02:47<01:29,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118/179] img_00131.jpeg\n",
      "  Base: a man in a suit and tie standing in a dark room\n",
      "  VQA: a man in a suit and tie standing in a dark room\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  66%|██████▋   | 119/179 [02:48<01:27,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119/179] img_00132.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 120/179 [02:50<01:25,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120/179] img_00133.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  68%|██████▊   | 121/179 [02:51<01:23,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121/179] img_00134.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  68%|██████▊   | 122/179 [02:53<01:20,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122/179] img_00135.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a car parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  69%|██████▊   | 123/179 [02:54<01:17,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123/179] img_00136.jpeg\n",
      "  Base: a dark brown metal texture background\n",
      "  VQA: a dark brown background with some light spots\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  69%|██████▉   | 124/179 [02:56<01:19,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124/179] img_00137.jpeg\n",
      "  Base: a man in a suit and tie standing in front of a window\n",
      "  VQA: a man in a suit standing in front of a black background\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|██████▉   | 125/179 [02:57<01:18,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125/179] img_00139.jpeg\n",
      "  Base: a light shining through a window in a dark room\n",
      "  VQA: a light shines through a window in a dark room\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|███████   | 126/179 [02:58<01:15,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[126/179] img_00140.jpeg\n",
      "  Base: a heart shaped hole in the wall of a building\n",
      "  VQA: a heart shaped hole in the wall of a building\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  71%|███████   | 127/179 [03:00<01:14,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127/179] img_00141.jpeg\n",
      "  Base: a heart shaped shadow on the wall of a building\n",
      "  VQA: an image of a heart in the middle of a wall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|███████▏  | 128/179 [03:01<01:11,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128/179] img_00142.jpeg\n",
      "  Base: a black and white striped background\n",
      "  VQA: an abstract black and white background with vertical lines\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|███████▏  | 129/179 [03:03<01:09,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[129/179] img_00143.jpeg\n",
      "  Base: an image of the moon in the night sky\n",
      "  VQA: an image of the moon in the night sky\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 130/179 [03:04<01:10,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130/179] img_00144.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car that is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 131/179 [03:05<01:08,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[131/179] img_00145.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a black car parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  74%|███████▎  | 132/179 [03:07<01:09,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132/179] img_00146.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a car that is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  74%|███████▍  | 133/179 [03:09<01:08,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[133/179] img_00147.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car that is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▍  | 134/179 [03:10<01:05,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134/179] img_00148.jpeg\n",
      "  Base: a close up of a black and red curtain\n",
      "  VQA: a black and red striped background with a white stripe\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▌  | 135/179 [03:12<01:13,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[135/179] img_00149.jpeg\n",
      "  Base: a man in a suit and tie walking down a street\n",
      "  VQA: a man in a suit and tie walking down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  76%|███████▌  | 136/179 [03:14<01:08,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136/179] img_00150.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 137/179 [03:15<01:05,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137/179] img_00151.jpeg\n",
      "  Base: a person riding a skateboard down a street\n",
      "  VQA: a person riding a skateboard down a city street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 138/179 [03:16<01:01,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138/179] img_00152.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a black car parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  78%|███████▊  | 139/179 [03:18<00:59,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139/179] img_00153.jpeg\n",
      "  Base: an empty street in the middle of a city\n",
      "  VQA: an empty street in the middle of a city\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  78%|███████▊  | 140/179 [03:19<00:58,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140/179] img_00154.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a person in a dark room\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  79%|███████▉  | 141/179 [03:21<00:55,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141/179] img_00155.jpeg\n",
      "  Base: a black car parked on the side of a road\n",
      "  VQA: a black car parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  79%|███████▉  | 142/179 [03:22<00:53,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142/179] img_00156.jpeg\n",
      "  Base: a car parked on the side of a road\n",
      "  VQA: a white van parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|███████▉  | 143/179 [03:24<00:51,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143/179] img_00157.jpeg\n",
      "  Base: a man riding a motorcycle down a city street\n",
      "  VQA: a man riding a motorcycle down a city street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|████████  | 144/179 [03:25<00:49,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144/179] img_00160.jpeg\n",
      "  Base: a red, white and blue striped background\n",
      "  VQA: a red, white and blue striped background\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████  | 145/179 [03:26<00:47,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145/179] img_00161.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: an image of a black and white striped wallpaper\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  82%|████████▏ | 146/179 [03:28<00:47,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[146/179] img_00162.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white image of a man's face\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  82%|████████▏ | 147/179 [03:29<00:46,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147/179] img_00163.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 148/179 [03:31<00:44,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[148/179] img_00164.jpeg\n",
      "  Base: an abstract image of trees in the woods\n",
      "  VQA: an abstract image of a tree in the woods\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 149/179 [03:32<00:42,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149/179] img_00165.jpeg\n",
      "  Base: a man is standing in the middle of a road\n",
      "  VQA: a hole in the ground that has been dug\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  84%|████████▍ | 150/179 [03:34<00:41,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150/179] img_00168.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  84%|████████▍ | 151/179 [03:35<00:40,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151/179] img_00169.jpeg\n",
      "  Base: a motorcycle parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▍ | 152/179 [03:37<00:39,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[152/179] img_00170.jpeg\n",
      "  Base: a black and white photograph of a man in a suit\n",
      "  VQA: a black and white photograph of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▌ | 153/179 [03:38<00:37,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[153/179] img_00171.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  86%|████████▌ | 154/179 [03:40<00:36,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[154/179] img_00172.jpeg\n",
      "  Base: a blue car parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 155/179 [03:41<00:34,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155/179] img_00173.jpeg\n",
      "  Base: a person walking down a sidewalk next to a building\n",
      "  VQA: a person walking down a sidewalk next to a building\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 156/179 [03:42<00:33,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156/179] img_00174.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car is parked on the side of the road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 157/179 [03:44<00:32,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157/179] img_00175.jpeg\n",
      "  Base: a person riding a skateboard down a street\n",
      "  VQA: a man riding a skateboard down a city street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 158/179 [03:45<00:30,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158/179] img_00177.jpeg\n",
      "  Base: a dog is standing on the sidewalk in the dark\n",
      "  VQA: a black and white dog laying on the ground\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  89%|████████▉ | 159/179 [03:47<00:29,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[159/179] img_00178.jpeg\n",
      "  Base: a broken umbrella sitting on the side of a road\n",
      "  VQA: a street sign sitting on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  89%|████████▉ | 160/179 [03:48<00:28,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[160/179] img_00179.jpeg\n",
      "  Base: a close up of a metal tube\n",
      "  VQA: a close up of a metal tube with a white background\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|████████▉ | 161/179 [03:50<00:26,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161/179] img_00180.jpeg\n",
      "  Base: a man riding a skateboard down a street\n",
      "  VQA: a person riding on a skateboard down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  91%|█████████ | 162/179 [03:51<00:24,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[162/179] img_00181.jpeg\n",
      "  Base: a man in a suit and tie walking down the street\n",
      "  VQA: a man in a suit and tie is walking down the stairs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  91%|█████████ | 163/179 [03:53<00:23,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[163/179] img_00182.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 164/179 [03:54<00:21,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164/179] img_00183.jpeg\n",
      "  Base: a man in a suit and tie walking down a street\n",
      "  VQA: a man in a suit and tie walking down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 165/179 [03:56<00:20,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165/179] img_00184.jpeg\n",
      "  Base: a man in a suit and tie standing in front of a curtain\n",
      "  VQA: a man in a suit and tie standing in front of a curtain\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|█████████▎| 166/179 [03:57<00:19,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166/179] img_00185.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a man in a suit and tie standing on a stage\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|█████████▎| 167/179 [03:59<00:17,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[167/179] img_00187.jpeg\n",
      "  Base: a dark room with a wooden floor and a window\n",
      "  VQA: a dark room with a wooden floor and a large window\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  94%|█████████▍| 168/179 [04:00<00:15,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168/179] img_00188.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  94%|█████████▍| 169/179 [04:01<00:13,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169/179] img_00189.jpeg\n",
      "  Base: a man riding a skateboard down a street\n",
      "  VQA: a man riding a skateboard down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  95%|█████████▍| 170/179 [04:03<00:12,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170/179] img_00190.jpeg\n",
      "  Base: an image of a black and green striped background\n",
      "  VQA: a close up of a black and green striped background\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  96%|█████████▌| 171/179 [04:04<00:10,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171/179] img_00191.jpeg\n",
      "  Base: a person riding a skateboard down a street\n",
      "  VQA: a person riding a skateboard down a street\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  96%|█████████▌| 172/179 [04:05<00:09,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172/179] img_00192.jpeg\n",
      "  Base: a motorcycle parked on the side of a road\n",
      "  VQA: a motorcycle parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  97%|█████████▋| 173/179 [04:07<00:08,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173/179] img_00193.jpeg\n",
      "  Base: a motorcycle parked on the side of a dirt road\n",
      "  VQA: a motorcycle parked on the side of a dirt road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  97%|█████████▋| 174/179 [04:08<00:07,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[174/179] img_00194.jpeg\n",
      "  Base: a car is parked on the side of the road\n",
      "  VQA: a car parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 175/179 [04:10<00:05,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[175/179] img_00195.jpeg\n",
      "  Base: a car is parked in a parking lot\n",
      "  VQA: a car parked on the side of a road\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 176/179 [04:11<00:04,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[176/179] img_00196.jpeg\n",
      "  Base: a car driving down a street at night\n",
      "  VQA: a car driving down a dark street at night\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  99%|█████████▉| 177/179 [04:12<00:02,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[177/179] img_00197.jpeg\n",
      "  Base: a white car parked in a parking lot\n",
      "  VQA: a white car is parked in a parking lot\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  99%|█████████▉| 178/179 [04:14<00:01,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178/179] img_00198.jpeg\n",
      "  Base: a black and white photo of a man in a suit\n",
      "  VQA: a black and white photo of a man in a suit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 179/179 [04:15<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[179/179] img_00199.jpeg\n",
      "  Base: a man riding a bike down a dirt road\n",
      "  VQA: a man riding a bike down a dirt road\n",
      "\n",
      "Done. Results saved to: /home/jupyter/project/Ilya/multimodel_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# multimodel_llava_pipeline.py\n",
    "# Объединяет: rotation correction (resnet50), object detection (DETR / optional YOLO),\n",
    "# environment crop (нижняя часть под машиной), BLIP captioning + optional BLIP VQA (если есть).\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "# Transformers / BLIP\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, DetrImageProcessor, DetrForObjectDetection\n",
    "\n",
    "# Optional modules\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    _HAS_YOLO = True\n",
    "except Exception:\n",
    "    YOLO = None\n",
    "    _HAS_YOLO = False\n",
    "\n",
    "# Optional BLIP VQA from local BLIP repo (if installed via pip from github)\n",
    "try:\n",
    "    # from models.blip_vqa import blip_vqa  # this will work if BLIP repo is in python path\n",
    "    from models.blip_vqa import blip_vqa  # type: ignore\n",
    "    _HAS_BLIP_VQA = True\n",
    "except Exception:\n",
    "    blip_vqa = None\n",
    "    _HAS_BLIP_VQA = False\n",
    "\n",
    "# Optional rotation model (resnet50)\n",
    "try:\n",
    "    import torch.nn as nn\n",
    "    from torchvision import models, transforms\n",
    "    _HAS_TORCHVISION = True\n",
    "except Exception:\n",
    "    _HAS_TORCHVISION = False\n",
    "\n",
    "# ========== Настройки (отредактируйте пути) ==========\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "IMAGE_FOLDER = \"/home/jupyter/project/Ilya/dataset_images\"  # <- поменяйте под себя\n",
    "OUTPUT_JSON = \"/home/jupyter/project/Ilya/multimodel_results.json\"\n",
    "\n",
    "# Если локально есть модель поворота (resnet50 trained with 4 classes 0/90/180/270)\n",
    "ROTATION_MODEL_PATH = \"../MODELS/resnet50_rotation_car_99.76.pth\"  # если нет — пропустит шаг поворота\n",
    "\n",
    "# Если у вас есть yolov8 модель (опционально) — укажите путь\n",
    "YOLO_MODEL_PATH = \"../MODELS/yolov8x-oiv7.pt\"  # optional\n",
    "\n",
    "# DETR threshold\n",
    "DETR_THRESHOLD = 0.5\n",
    "\n",
    "# Какие COCO классы считаем \"машиной\"\n",
    "VEHICLE_CLASSES = {\"car\", \"truck\", \"bus\", \"motorcycle\", \"train\", \"van\", \"taxi\"}\n",
    "\n",
    "# Вопрос для VQA (пример)\n",
    "QUESTION = \"Describe only the surroundings near the car, such as buildings, trees, street lights, signs, or pavement. Do not mention the car, vehicle, sky, or clouds.\"\n",
    "\n",
    "# ======================================================\n",
    "\n",
    "# ========== Загрузка моделей трансформеров ==========\n",
    "print(\"Loading BLIP captioning...\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(DEVICE)\n",
    "\n",
    "print(\"Loading DETR (object detection)...\")\n",
    "detr_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\").to(DEVICE)\n",
    "\n",
    "# Попытка загрузить YOLO, если есть и файл указан\n",
    "yolo_model = None\n",
    "if _HAS_YOLO and YOLO is not None and os.path.exists(YOLO_MODEL_PATH):\n",
    "    try:\n",
    "        print(\"Loading YOLO model (optional)...\")\n",
    "        yolo_model = YOLO(YOLO_MODEL_PATH)\n",
    "    except Exception as e:\n",
    "        print(\"Не удалось загрузить YOLO:\", e)\n",
    "        yolo_model = None\n",
    "else:\n",
    "    if _HAS_YOLO:\n",
    "        print(\"YOLO установлен, но путь к модели не найден или модель не указана:\", YOLO_MODEL_PATH)\n",
    "    else:\n",
    "        print(\"ultralytics (YOLO) не установлен — пропускаем.\")\n",
    "\n",
    "# Попытка загрузить BLIP VQA (опционально)\n",
    "blip_vqa_model = None\n",
    "if _HAS_BLIP_VQA and blip_vqa is not None:\n",
    "    try:\n",
    "        print(\"Loading BLIP VQA (optional) from BLIP repo...\")\n",
    "        # пример download URL - если вы хотите использовать готовую pretrained weight, укажите его как в вашем примере\n",
    "        # model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
    "        # model = blip_vqa(pretrained=model_url, image_size=480, vit='base', med_config=...)\n",
    "        # В этом обобщённом скрипте попытаемся создать модель без weights (пользователь обязан настроить)\n",
    "        blip_vqa_model = None\n",
    "        # Если вы хотите использовать BLIP VQA — загрузите вручную модель и замените blip_vqa_model = model\n",
    "        print(\"BLIP VQA доступен, но автоматическая загрузка весов не настроена. Если хотите — укажите путь/URL и раскомментируйте загрузку.\")\n",
    "    except Exception as e:\n",
    "        print(\"BLIP VQA не удалось инициализировать:\", e)\n",
    "        blip_vqa_model = None\n",
    "else:\n",
    "    print(\"BLIP VQA недоступен (BLIP repo не найден). VQA будет выполнен через BLIP captioning fallback.\")\n",
    "\n",
    "# ========== Rotation model (опционально) ==========\n",
    "rotation_model = None\n",
    "rotation_transform = None\n",
    "angle_values = {0: 0, 1: 90, 2: 180, 3: 270}\n",
    "\n",
    "if _HAS_TORCHVISION and os.path.exists(ROTATION_MODEL_PATH):\n",
    "    try:\n",
    "        print(\"Loading rotation model (resnet50)...\")\n",
    "        rotation_model = models.resnet50(weights=None)\n",
    "        num_features = rotation_model.fc.in_features\n",
    "        rotation_model.fc = nn.Linear(num_features, 4)\n",
    "        rotation_model.load_state_dict(torch.load(ROTATION_MODEL_PATH, map_location=DEVICE))\n",
    "        rotation_model = rotation_model.to(DEVICE)\n",
    "        rotation_model.eval()\n",
    "\n",
    "        rotation_transform = transforms = __import__(\"torchvision.transforms\", fromlist=[\"transforms\"]).transforms.Compose([\n",
    "            __import__(\"torchvision.transforms\", fromlist=[\"transforms\"]).transforms.Resize((224, 224)),\n",
    "            __import__(\"torchvision.transforms\", fromlist=[\"transforms\"]).transforms.ToTensor(),\n",
    "            __import__(\"torchvision.transforms\", fromlist=[\"transforms\"]).transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                           std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(\"Не удалось загрузить rotation model:\", e)\n",
    "        rotation_model = None\n",
    "else:\n",
    "    print(\"Rotation model не найден или torchvision отсутствует — шаг поворота пропускается.\")\n",
    "\n",
    "# ========== Функции обработки ==========\n",
    "def correct_rotation_if_available(pil_image: Image.Image) -> Image.Image:\n",
    "    \"\"\"Если модель поворота доступна, предсказывает класс и разворачивает изображение.\"\"\"\n",
    "    if rotation_model is None or rotation_transform is None:\n",
    "        return pil_image\n",
    "    try:\n",
    "        input_tensor = rotation_transform(pil_image).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = rotation_model(input_tensor)\n",
    "            pred = int(torch.argmax(out, dim=1).item())\n",
    "        angle = angle_values.get(pred, 0)\n",
    "        if angle != 0:\n",
    "            return pil_image.rotate(-angle, expand=True)\n",
    "        return pil_image\n",
    "    except Exception as e:\n",
    "        print(\"Rotation correction failed:\", e)\n",
    "        return pil_image\n",
    "\n",
    "def detect_vehicles_detr(pil_image: Image.Image, threshold: float = DETR_THRESHOLD) -> List[np.ndarray]:\n",
    "    \"\"\"Возвращает список bbox'ов (xmin, ymin, xmax, ymax) обнаруженных как vehicles по DETR.\"\"\"\n",
    "    try:\n",
    "        inputs = detr_processor(images=pil_image, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = detr_model(**inputs)\n",
    "        target_sizes = torch.tensor([pil_image.size[::-1]]).to(DEVICE)\n",
    "        results = detr_processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=threshold)[0]\n",
    "        boxes = []\n",
    "        for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "            class_name = detr_model.config.id2label[int(label.item())]\n",
    "            if class_name.lower() in VEHICLE_CLASSES:\n",
    "                boxes.append(box.cpu().numpy())\n",
    "        return boxes\n",
    "    except Exception as e:\n",
    "        print(\"DETR detection failed:\", e)\n",
    "        return []\n",
    "\n",
    "def detect_vehicles_yolo(pil_image: Image.Image) -> List[np.ndarray]:\n",
    "    \"\"\"Если есть yolov8, возвращает list of boxes (xmin, ymin, xmax, ymax) для vehicle-like classes.\"\"\"\n",
    "    if yolo_model is None:\n",
    "        return []\n",
    "    try:\n",
    "        results = yolo_model(pil_image, verbose=False)\n",
    "        r = results[0]\n",
    "        boxes = []\n",
    "        for box in r.boxes:\n",
    "            cls_id = int(box.cls)\n",
    "            cls_name = r.names[cls_id].lower()\n",
    "            if cls_name in VEHICLE_CLASSES or \"car\" in cls_name or \"vehicle\" in cls_name:\n",
    "                xyxy = box.xyxy.cpu().numpy().tolist()  # [xmin, ymin, xmax, ymax]\n",
    "                boxes.append(np.array(xyxy))\n",
    "        return boxes\n",
    "    except Exception as e:\n",
    "        print(\"YOLO detection failed:\", e)\n",
    "        return []\n",
    "\n",
    "def crop_to_environment_from_boxes(pil_image: Image.Image, vehicle_boxes: List[np.ndarray]) -> Image.Image:\n",
    "    \"\"\"Обрезает изображение, оставляя область под самым нижним автомобилем (или нижнюю половину, если нет box).\"\"\"\n",
    "    arr = np.array(pil_image)\n",
    "    h, w = arr.shape[:2]\n",
    "    if not vehicle_boxes:\n",
    "        return Image.fromarray(arr[h//2:, :])\n",
    "\n",
    "    # Найдём самый нижний y_max\n",
    "    ymaxs = [box[3] for box in vehicle_boxes]\n",
    "    max_ymax = max(ymaxs)\n",
    "    crop_y1 = int(max_ymax)\n",
    "    if crop_y1 >= h:\n",
    "        crop_y1 = h // 2\n",
    "    cropped = arr[crop_y1:, :]\n",
    "    if cropped.size == 0:\n",
    "        cropped = arr[h//2:, :]\n",
    "    return Image.fromarray(cropped)\n",
    "\n",
    "def generate_captions(env_image: Image.Image) -> Dict[str, str]:\n",
    "    \"\"\"Генерирует три варианта подписи через BLIP.\"\"\"\n",
    "    try:\n",
    "        inputs = blip_processor(env_image, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = blip_model.generate(**inputs, max_length=50, num_beams=5, early_stopping=True)\n",
    "            base = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            detailed_outputs = blip_model.generate(**inputs, max_length=100, num_beams=7, length_penalty=2.0, early_stopping=True)\n",
    "            detailed = blip_processor.decode(detailed_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            alt_outputs = blip_model.generate(**inputs, max_length=60, do_sample=True, temperature=0.9, top_p=0.9)\n",
    "            alt = blip_processor.decode(alt_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        return {\"base\": base, \"detailed\": detailed, \"alternative\": alt}\n",
    "    except Exception as e:\n",
    "        print(\"BLIP caption failed:\", e)\n",
    "        return {\"base\": \"Ошибка\", \"detailed\": \"Ошибка\", \"alternative\": \"Ошибка\"}\n",
    "\n",
    "def answer_vqa(env_image: Image.Image, question: str) -> str:\n",
    "    \"\"\"Пытается ответить на вопрос. Если BLIP VQA доступен — использует её, иначе выполняет fallback (BLIP caption + prompt).\"\"\"\n",
    "    # 1) Если BLIP VQA модель доступна (пользователь должен настроить загрузку) — вызвать её.\n",
    "    if blip_vqa_model is not None:\n",
    "        try:\n",
    "            # Placeholder: конкретный API зависит от реализации blip_vqa interface\n",
    "            # Пример: answer = blip_vqa_model(image_tensor, [question], train=False, inference='generate')\n",
    "            # return answer[0]\n",
    "            return \"VQA через BLIP VQA (локально) — ответ не настроен в примере.\"\n",
    "        except Exception as e:\n",
    "            print(\"BLIP VQA failed:\", e)\n",
    "\n",
    "    # 2) Fallback: сконкатенировать question с prompt и прогнать BLIP captioning (генеративно)\n",
    "    try:\n",
    "        # Для более качественного ответа формируем подсказку: добавим instruction + question\n",
    "        instruction = f\"Answer the question about the environment near the car. Question: {question}\"\n",
    "        # BLIP captioning expects just an image; мы попробуем передать instruction в decoder как prompt (форсируем через input_ids)\n",
    "        # Простая стратегия: сгенерировать несколько caption'ов и вернуть наиболее подходящий (detailed)\n",
    "        caps = generate_captions(env_image)\n",
    "        # Простейший \"fake VQA\": вернуть detailed caption — пользователь может захотеть более точную реализацию\n",
    "        return caps[\"detailed\"]\n",
    "    except Exception as e:\n",
    "        print(\"VQA fallback failed:\", e)\n",
    "        return \"Ошибка при попытке ответить на VQA.\"\n",
    "\n",
    "# ========== Main loop ==========\n",
    "def process_folder(image_folder: str, output_json: str):\n",
    "    image_folder = Path(image_folder)\n",
    "    assert image_folder.exists(), f\"Папка не найдена: {image_folder}\"\n",
    "\n",
    "    image_extensions = {'.png', '.jpg', '.jpeg', '.bmp'}\n",
    "    files = [p for p in sorted(image_folder.iterdir()) if p.suffix.lower() in image_extensions]\n",
    "    print(f\"Найдено изображений: {len(files)}\")\n",
    "\n",
    "    results: Dict[str, Any] = {}\n",
    "\n",
    "    for i, p in enumerate(tqdm(files, desc=\"Processing\")):\n",
    "        fname = p.name\n",
    "        try:\n",
    "            pil = Image.open(p).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot open {p}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 1) rotation\n",
    "        pil_rot = correct_rotation_if_available(pil)\n",
    "\n",
    "        # 2) detect vehicles (yolo optional, then DETR fallback)\n",
    "        boxes = []\n",
    "        if yolo_model is not None:\n",
    "            boxes = detect_vehicles_yolo(pil_rot)\n",
    "        if not boxes:\n",
    "            boxes = detect_vehicles_detr(pil_rot)\n",
    "\n",
    "        # 3) crop environment\n",
    "        env_img = crop_to_environment_from_boxes(pil_rot, boxes)\n",
    "\n",
    "        # 4) captions\n",
    "        captions = generate_captions(env_img)\n",
    "\n",
    "        # 5) vqa\n",
    "        vqa_answer = answer_vqa(env_img, QUESTION)\n",
    "\n",
    "        # 6) aggregate\n",
    "        results[fname] = {\n",
    "            \"captions\": captions,\n",
    "            \"vqa_answer\": vqa_answer,\n",
    "            \"detected_vehicle_boxes\": [box.tolist() for box in boxes],\n",
    "            \"rotated\": pil_rot.size if pil_rot is not None else None,\n",
    "            \"env_crop_size\": env_img.size\n",
    "        }\n",
    "\n",
    "        # краткий прогресс в stdout\n",
    "        print(f\"[{i+1}/{len(files)}] {fname}\")\n",
    "        print(\"  Base:\", captions[\"base\"])\n",
    "        print(\"  VQA:\", vqa_answer)\n",
    "        print()\n",
    "\n",
    "    # сохранить json\n",
    "    os.makedirs(os.path.dirname(output_json), exist_ok=True)\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Done. Results saved to:\", output_json)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_folder(IMAGE_FOLDER, OUTPUT_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b9467e1-1d52-4c8f-a75a-60037e81510d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T14:34:31.210911Z",
     "iopub.status.busy": "2025-10-16T14:34:31.209896Z",
     "iopub.status.idle": "2025-10-16T14:34:49.191057Z",
     "shell.execute_reply": "2025-10-16T14:34:49.190118Z",
     "shell.execute_reply.started": "2025-10-16T14:34:31.210876Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: timm in /home/jupyter/.local/lib/python3.10/site-packages (0.4.12)\n",
      "Requirement already satisfied: torchvision in /home/jupyter/.local/lib/python3.10/site-packages (0.16.2)\n",
      "Requirement already satisfied: torch>=1.4 in /home/jupyter/.local/lib/python3.10/site-packages (from timm) (2.1.2+cu118)\n",
      "Requirement already satisfied: numpy in /home/jupyter/.local/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.4->timm) (4.15.0)\n",
      "Requirement already satisfied: sympy in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.4->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (2023.6.0)\n",
      "Collecting triton==2.1.0 (from torch>=1.4->timm)\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm) (2.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm) (1.3.0)\n",
      "Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: triton\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.2+cu118 which is incompatible.\n",
      "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.1.2+cu118 which is incompatible.\n",
      "torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.1.2+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed triton-2.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install timm torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "296bf77d-27ef-4f67-a7fc-e1f22eab8074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T14:34:49.193886Z",
     "iopub.status.busy": "2025-10-16T14:34:49.192875Z",
     "iopub.status.idle": "2025-10-16T14:34:55.830029Z",
     "shell.execute_reply": "2025-10-16T14:34:55.829067Z",
     "shell.execute_reply.started": "2025-10-16T14:34:49.193842Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /home/jupyter/.local/lib/python3.10/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b4b27af-09f1-45ed-883d-4fdea7d8ae72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T14:34:01.923934Z",
     "iopub.status.busy": "2025-10-16T14:34:01.922938Z",
     "iopub.status.idle": "2025-10-16T14:34:01.961175Z",
     "shell.execute_reply": "2025-10-16T14:34:01.960147Z",
     "shell.execute_reply.started": "2025-10-16T14:34:01.923890Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%python -c \"import transformers; print(transformers.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c1beaec-f302-4082-a1ae-2aaa87990540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T14:35:20.114734Z",
     "iopub.status.busy": "2025-10-16T14:35:20.113655Z",
     "iopub.status.idle": "2025-10-16T14:35:20.292414Z",
     "shell.execute_reply": "2025-10-16T14:35:20.291038Z",
     "shell.execute_reply.started": "2025-10-16T14:35:20.114692Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.mask2former.image_processing_mask2former because of the following error (look up to see its traceback):\nNo module named 'transformers.models.mask2former.image_processing_mask2former'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_ipex_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_version\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mget_major_and_minor_from_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.mask2former.image_processing_mask2former'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3901/653735843.py\u001b[0m in \u001b[0;36m<cell line: 145>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3901/653735843.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# --- Основной run: перебор изображений, запуск pipeline, запуск llava и сохранение результата ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mrun_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_run_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_llava\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Список картинок\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3901/653735843.py\u001b[0m in \u001b[0;36mload_run_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Выполним код ноутбука в изолированном namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNOTEBOOK_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"run_pipeline\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_pipeline\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/github/car-scene-captioning/main.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m {\n\u001b[1;32m      2\u001b[0m  \"cells\": [\n\u001b[0;32m----> 3\u001b[0;31m   {\n\u001b[0m\u001b[1;32m      4\u001b[0m    \u001b[0;34m\"cell_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"code\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0;34m\"execution_count\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/github/car-scene-captioning/autocaption/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_processor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageRotator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCarDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mObjectExtractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSceneExtractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPhotoDescriber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPhotoDescriberWithQuestion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtext_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/github/car-scene-captioning/autocaption/feature_extractor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlipProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlipForConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMask2FormerImageProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMask2FormerForUniversalSegmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mninja\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mninja\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbuild\u001b[0m \u001b[0msystem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mavailable\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \"\"\"\n\u001b[0;32m-> 1101\u001b[0;31m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ninja\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--version\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0mCode\u001b[0m \u001b[0mcomes\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_extension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ninja_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mReturns\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mninja\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mninja\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbuild\u001b[0m \u001b[0msystem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mavailable\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ninja\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--version\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_major_and_minor_from_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ipex_available\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.mask2former.image_processing_mask2former because of the following error (look up to see its traceback):\nNo module named 'transformers.models.mask2former.image_processing_mask2former'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# llava_integration.py\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# --- Настройки ---\n",
    "IMAGES_DIR = Path(\"/home/jupyter/project/Ilya/dataset_images\")\n",
    "OUTPUT_DIR = Path(\"/home/jupyter/project/Ilya/llava_outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Путь к ноутбуку/модулю с run_pipeline\n",
    "NOTEBOOK_PATH = Path(\"/home/jupyter/project/github/car-scene-captioning/main.ipynb\")\n",
    "# или если вы скопировали/создали run_pipeline.py рядом - укажите его:\n",
    "RUNTIME_MODULE_PATH = Path(\"/home/jupyter/project/github/car-scene-captioning/run_pipeline.py\")\n",
    "\n",
    "# Модель LLaVA — пример идентификатора; поменяйте на нужный\n",
    "LLAVA_MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- helper: получить run_pipeline из .py или .ipynb ---\n",
    "def load_run_pipeline():\n",
    "    # 1) Попробовать импортировать из .py модуля если есть\n",
    "    if RUNTIME_MODULE_PATH.exists():\n",
    "        import importlib.util\n",
    "        spec = importlib.util.spec_from_file_location(\"run_pipeline_module\", str(RUNTIME_MODULE_PATH))\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        if hasattr(module, \"run_pipeline\"):\n",
    "            return module.run_pipeline\n",
    "\n",
    "    # 2) Если есть ноутбук, собрать и выполнить код его кодовых ячеек\n",
    "    if NOTEBOOK_PATH.exists():\n",
    "        import nbformat\n",
    "        nb = nbformat.read(str(NOTEBOOK_PATH), as_version=4)\n",
    "        code_cells = [c.source for c in nb.cells if c.cell_type == \"code\"]\n",
    "        code = \"\\n\\n\".join(code_cells)\n",
    "        ns = {}\n",
    "        # Выполним код ноутбука в изолированном namespace\n",
    "        exec(compile(code, str(NOTEBOOK_PATH), \"exec\"), ns)\n",
    "        if \"run_pipeline\" in ns:\n",
    "            return ns[\"run_pipeline\"]\n",
    "\n",
    "    raise RuntimeError(\"Не удалось найти функцию run_pipeline ни в run_pipeline.py, ни в main.ipynb. Скопируйте/экспортируйте её в доступный модуль.\")\n",
    "\n",
    "# --- Загрузка LLaVA (inference) ---\n",
    "def load_llava(model_id=LLAVA_MODEL_ID, device=DEVICE):\n",
    "    # попытка импортировать типичный API, используемый в примерах LLaVA\n",
    "    try:\n",
    "        # некоторые реализации предоставляют классы напрямую\n",
    "        from llava.modeling import LlavaForConditionalGeneration\n",
    "        from transformers import AutoProcessor\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "    except Exception:\n",
    "        # fallback — часто можно импортировать напрямую из transformers or llava-hf wrappers\n",
    "        try:\n",
    "            from transformers import AutoProcessor\n",
    "            from llava import LlavaForConditionalGeneration  # возможные импорты в разных реализациях\n",
    "            model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "            processor = AutoProcessor.from_pretrained(model_id)\n",
    "        except Exception:\n",
    "            # более общий вариант — использовать AutoModel/AutoTokenizer (пользователь может поправить по своему стеку)\n",
    "            from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "            processor = AutoProcessor.from_pretrained(model_id)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return processor, model\n",
    "\n",
    "# --- Формирование промпта: комбинируем результаты вашего pipeline и вопрос для LLaVA ---\n",
    "def build_prompt(pipeline_result: list):\n",
    "    # pipeline_result — список строк/словарей, которые возвращает ваш run_pipeline для одного изображения\n",
    "    # Пример шаблона — вы можете адаптировать под задачу:\n",
    "    header = \"Информация, полученная preprocessing pipeline:\\n\"\n",
    "    body_lines = []\n",
    "    for item in pipeline_result:\n",
    "        # если это словарь — представим его красиво\n",
    "        if isinstance(item, dict):\n",
    "            body_lines.append(json.dumps(item, ensure_ascii=False))\n",
    "        else:\n",
    "            body_lines.append(str(item))\n",
    "    body = \"\\n\".join(body_lines)\n",
    "    # вопрос/задача для LLaVA\n",
    "    question = (\n",
    "        \"\\n\\nЗадача: на основании изображения и информации выше опишите кратко автомобиль, \"\n",
    "        \"окрестность и перечислите обнаруженные объекты. Выдайте ответ в двух частях:\\n\"\n",
    "        \"1) Краткая сводка (1-2 предложения)\\n\"\n",
    "        \"2) JSON с полями: objects (список), scene (строка), potential_hazards (строка, если есть)\\n\"\n",
    "    )\n",
    "    return header + body + question\n",
    "\n",
    "# --- Основной run: перебор изображений, запуск pipeline, запуск llava и сохранение результата ---\n",
    "def main():\n",
    "    run_pipeline = load_run_pipeline()\n",
    "    processor, model = load_llava()\n",
    "    # Список картинок\n",
    "    image_paths = sorted([p for p in IMAGES_DIR.iterdir() if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]])\n",
    "\n",
    "    batch_paths = [str(p) for p in image_paths]\n",
    "    # Запускаем ваш pipeline (он сам инициализирует модели внутри себя)\n",
    "    print(\"Запуск вашего run_pipeline для всех изображений...\")\n",
    "    pipeline_results = run_pipeline(batch_paths, source=True)  # ожидается list[list[...]]\n",
    "    print(\"Pipeline завершён.\")\n",
    "\n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # pipeline_results[idx] — список результатов для конкретного изображения\n",
    "        prompt = build_prompt(pipeline_results[idx])\n",
    "\n",
    "        # Подготовка входа для LLaVA: processor соединит изображение и текст\n",
    "        inputs = processor(images=img, text=prompt, return_tensors=\"pt\")\n",
    "        # Переносим на устройство\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "        # Генерация — параметры подберите по ресурсам\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(**inputs, max_new_tokens=256, do_sample=False, num_beams=4)\n",
    "\n",
    "        # Декодирование — у processor/токенайзера может быть метод decode\n",
    "        try:\n",
    "            generated_text = processor.tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "        except Exception:\n",
    "            # fallback если processor не содержит tokenizer\n",
    "            from transformers import AutoTokenizer\n",
    "            tok = AutoTokenizer.from_pretrained(LLAVA_MODEL_ID)\n",
    "            generated_text = tok.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "        # Сохраняем\n",
    "        out = {\n",
    "            \"image\": str(img_path),\n",
    "            \"pipeline\": pipeline_results[idx],\n",
    "            \"llava_output\": generated_text\n",
    "        }\n",
    "        out_path = OUTPUT_DIR / (img_path.stem + \"_llava.json\")\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Обработано {img_path.name} -> {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7822d3-5fd5-4791-9c60-88d35d8aac8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64bf180c-f548-4b12-8f50-58735894e69c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T14:36:04.100447Z",
     "iopub.status.busy": "2025-10-16T14:36:04.099145Z",
     "iopub.status.idle": "2025-10-16T14:36:04.145320Z",
     "shell.execute_reply": "2025-10-16T14:36:04.144588Z",
     "shell.execute_reply.started": "2025-10-16T14:36:04.100401Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/requirements.py\", line 35, in __init__\n",
      "    parsed = _parse_requirement(requirement_string)\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/_parser.py\", line 64, in parse_requirement\n",
      "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/_parser.py\", line 73, in _parse_requirement\n",
      "    name_token = tokenizer.expect(\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/_tokenizer.py\", line 140, in expect\n",
      "    raise self.raise_syntax_error(f\"Expected {expected}\")\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/_tokenizer.py\", line 165, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "ml_kernel._vendor.packaging._tokenizer.ParserSyntaxError: Expected package name at the start of dependency specifier\n",
      "    \"transformers>=4.35.0\"\n",
      "    ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/magics/pip.py\", line 64, in pip\n",
      "    command = self.build_command(line)\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/magics/pip.py\", line 105, in build_command\n",
      "    self._validate_pkgs(pkgs, pip_cmd == 'uninstall')\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/magics/pip.py\", line 50, in _validate_pkgs\n",
      "    pkg = Requirement(name)\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/requirements.py\", line 37, in __init__\n",
      "    raise InvalidRequirement(str(e)) from e\n",
      "ml_kernel._vendor.packaging.requirements.InvalidRequirement: Expected package name at the start of dependency specifier\n",
      "    \"transformers>=4.35.0\"\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "%pip -q install --upgrade \"transformers>=4.35.0\" \"timm>=0.6.0\" \"torchvision>=0.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48f0cdc7-6b2d-49d5-bde9-2b1b05a666a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T14:14:53.886966Z",
     "iopub.status.busy": "2025-10-16T14:14:53.886064Z",
     "iopub.status.idle": "2025-10-16T14:17:09.612223Z",
     "shell.execute_reply": "2025-10-16T14:17:09.611447Z",
     "shell.execute_reply.started": "2025-10-16T14:14:53.886924Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /home/jupyter/.local/lib/python3.10/site-packages (4.30.0)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.13.3\n",
      "\u001b[2K    Uninstalling tokenizers-0.13.3:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.13.3\n",
      "\u001b[2K  Attempting uninstall: transformers━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [tokenizers]\n",
      "\u001b[2K    Found existing installation: transformers 4.30.02m0/2\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling transformers-4.30.0:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.30.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/jupyter/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llava 1.2.2.post1 requires timm==0.6.13, but you have timm 0.4.12 which is incompatible.\n",
      "llava 1.2.2.post1 requires tokenizers==0.15.1, but you have tokenizers 0.22.1 which is incompatible.\n",
      "llava 1.2.2.post1 requires transformers==4.37.2, but you have transformers 4.57.1 which is incompatible.\n",
      "salesforce-lavis 1.0.2 requires transformers<4.27,>=4.25.0, but you have transformers 4.57.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.22.1 transformers-4.57.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/requirements.py\", line 35, in __init__\n",
      "    parsed = _parse_requirement(requirement_string)\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/_parser.py\", line 64, in parse_requirement\n",
      "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/_parser.py\", line 73, in _parse_requirement\n",
      "    name_token = tokenizer.expect(\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/_tokenizer.py\", line 140, in expect\n",
      "    raise self.raise_syntax_error(f\"Expected {expected}\")\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/_tokenizer.py\", line 165, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "ml_kernel._vendor.packaging._tokenizer.ParserSyntaxError: Expected package name at the start of dependency specifier\n",
      "    \"transformers>=4.39.0\"\n",
      "    ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/magics/pip.py\", line 64, in pip\n",
      "    command = self.build_command(line)\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/magics/pip.py\", line 105, in build_command\n",
      "    self._validate_pkgs(pkgs, pip_cmd == 'uninstall')\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/magics/pip.py\", line 50, in _validate_pkgs\n",
      "    pkg = Requirement(name)\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/_vendor/packaging/requirements.py\", line 37, in __init__\n",
      "    raise InvalidRequirement(str(e)) from e\n",
      "ml_kernel._vendor.packaging.requirements.InvalidRequirement: Expected package name at the start of dependency specifier\n",
      "    \"transformers>=4.39.0\"\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "# обновить до последней стабильной\n",
    "%pip install -U transformers\n",
    "\n",
    "# (опционально) если хотите конкретную более новую версию:\n",
    "%pip install -U \"transformers>=4.39.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74f5e6f1-0dce-4d4d-ac06-a303931437be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T14:31:04.160314Z",
     "iopub.status.busy": "2025-10-16T14:31:04.159062Z",
     "iopub.status.idle": "2025-10-16T14:31:04.484894Z",
     "shell.execute_reply": "2025-10-16T14:31:04.483440Z",
     "shell.execute_reply.started": "2025-10-16T14:31:04.160275Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The virtual environment was not created successfully because ensurepip is not\n",
      "available.  On Debian/Ubuntu systems, you need to install the python3-venv\n",
      "package using the following command.\n",
      "\n",
      "    apt install python3.10-venv\n",
      "\n",
      "You may need to use sudo with that command.  After installing the python3-venv\n",
      "package, recreate your virtual environment.\n",
      "\n",
      "Failing command: /home/jupyter/venv_autocaption/bin/python3\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Process exited with code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3901/564605873.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# вариант 1: через \"!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python3 -m venv ~/venv_autocaption'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source ~/venv_autocaption/bin/activate && pip install --upgrade pip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source ~/venv_autocaption/bin/activate && pip install torch torchvision pillow numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/ml_kernel/kernel.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScriptExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_output_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_user_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/ml_kernel/script_executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, lang, code)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Process exited with code %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: Process exited with code 1"
     ]
    }
   ],
   "source": [
    "# вариант 1: через \"!\"\n",
    "!python3 -m venv ~/venv_autocaption\n",
    "!source ~/venv_autocaption/bin/activate && pip install --upgrade pip\n",
    "!source ~/venv_autocaption/bin/activate && pip install torch torchvision pillow numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8baaa826-a2f1-4c86-8fd0-e113cf622e73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T14:17:09.614174Z",
     "iopub.status.busy": "2025-10-16T14:17:09.613761Z",
     "iopub.status.idle": "2025-10-16T14:17:09.632633Z",
     "shell.execute_reply": "2025-10-16T14:17:09.631658Z",
     "shell.execute_reply.started": "2025-10-16T14:17:09.614144Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (763689901.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3901/763689901.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python - <<'PY'\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python - <<'PY'\n",
    "from transformers import Mask2FormerImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "print(\"Mask2Former импортируется ✓\")\n",
    "PY\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
